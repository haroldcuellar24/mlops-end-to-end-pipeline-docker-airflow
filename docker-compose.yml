services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5

  airflow-init:
    image: ${AIRFLOW_IMAGE_NAME}
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_CORE_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_CORE_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW_CORE_DAGS_ARE_PAUSED_AT_CREATION}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_CORE_LOAD_EXAMPLES}
      AIRFLOW__LOGGING__REMOTE_LOGGING: ${AIRFLOW_LOGGING_REMOTE_LOGGING}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY}
    entrypoint: >
      bash -c "
      airflow db init &&
      airflow users create --username ${AIRFLOW_USER} --firstname ${AIRFLOW_FIRSTNAME} --lastname ${AIRFLOW_LASTNAME} --role ${AIRFLOW_ROLE}  --email ${AIRFLOW_EMAIL} --password ${AIRFLOW_PASSWORD}
      "
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./img:/opt/airflow/img
      - ./models:/opt/airflow/models

  airflow:
    build: .
    image: ${AIRFLOW_IMAGE_NAME}
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_CORE_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_CORE_FERNET_KEY}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: ${AIRFLOW_CORE_DAGS_ARE_PAUSED_AT_CREATION}
      AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW_CORE_LOAD_EXAMPLES}
      AIRFLOW__LOGGING__REMOTE_LOGGING: ${AIRFLOW_LOGGING_REMOTE_LOGGING}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY}
      AIRFLOW_UID: ${AIRFLOW_UID}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./img:/opt/airflow/img
      - ./models:/opt/airflow/models
    ports:
      - "8080:8080"
    command: webserver

  scheduler:
    image: ${AIRFLOW_IMAGE_NAME}
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_CORE_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres/${POSTGRES_DB}
      AIRFLOW__LOGGING__REMOTE_LOGGING: ${AIRFLOW_LOGGING_REMOTE_LOGGING}
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY}
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./img:/opt/airflow/img
      - ./models:/opt/airflow/models
    command: scheduler

  api:
    build:
      context: ./api
    volumes:
      - ./models:/app/models         # Access to the trained model
    ports:
      - "5000:5000"              # Expose API on localhost:5000

  webserver:
    image: python:3.9-slim
    working_dir: /usr/src/app
    volumes:
      - ./static:/usr/src/app/ # Mount the static folder
      - ./img:/usr/src/app/img       # Mount the images generated by Airflow
      - ./models:/usr/src/app/models # Mount the models
    command: python -m http.server 8000
    ports:
      - "8000:8000"

volumes:
  postgres-db-volume: